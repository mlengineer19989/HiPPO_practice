# HiPPO_practice

RNN, CNN, Transformerに代わる新たな時系列予測モデルとして話題のHiPPO frameworkやStructured State Spaces(S4)について理解するためのリポジトリ。この技術の研究をしているGithubリポジトリ(https://github.com/HazyResearch/state-spaces)に色々情報が載っている。


## ベースの論文
3つの論文を順番に読み解く必要がある。上記リポジトリには、最新の論文情報も掲載。
1. HiPPO: Recurrent Memory with Optimal Polynomial Projections (https://arxiv.org/abs/2008.07669)
2. Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers (https://arxiv.org/abs/2110.13985)
3. Efficiently Modeling Long Sequences with Structured State Spaces (https://arxiv.org/abs/2111.00396)

## 参考になった解説サイト
- 時系列モデリング手法HiPPOを読み解く(https://dosuex.com/entry/2022/09/10/125807)
- Is Attention All You Need? Transformerを超える(?)新モデルS4(https://recruit.gmo.jp/engineer/jisedai/blog/is-attention-all-you-need/)
- 企業Morphoの輪読資料（これが一番わかりやすい）　(https://techblog.morphoinc.com/entry/2022/05/24/102648)